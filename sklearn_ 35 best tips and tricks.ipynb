{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Welcome to this Kernel\n\n## *** This kernel is a compilation of tricks of sklearn published by Kevin Markham weekly.***\n\nYou can find the original tricks and tips on the GitHub repo:\n\nhttps://github.com/justmarkham/scikit-learn-tips\n\n\n## *** This kernel is under construction. I will be updating it regularly.***","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"table_of_contents\"></a>\n# Table of contents\n\n[Importing libraries and setting some helper functions](#Imports)\n\n[Trick 35: Passing a df directly to sklearn](#trick35)\n\n[Trick 34: Feature selection with Pipeline](#trick34)\n\n[Trick 33: Using custom and existing function in a ColumnTransformer](#trick33)\n\n[Trick 32: Area Under Curve (AUC) for binary classification: ovo and ovr strategies](#trick32)\n\n[Trick 31: Shuffle when using cross_val_score](#trick31)\n\n[Trick 30: Four ways of displaying the model coefficients](#trick30)\n\n[Trick 29: Vectorize two text columns using ColumnTransformer](#trick29)\n\n[Trick 28: Save a model of pipeline using joblib](#trick28)\n\n[Trick 27: Imputing missing values for categorical values](#trick27)\n\n[Trick 26: Use of stratify when performing classification problems](#trick26)\n\n[Trick 25: Prunning decision trees (new in 0.22 and above).](#trick25)\n\n[Trick 24: Plotting the decision tree with sklearn (new in 0.21 and above)](#trick24)\n\n[Trick 23: Display the intercept & coefficients for a liner model](#trick23)\n\n[Trick 22: Two types of Pipelines](#trick22)\n\n[Trick 21: Several ROC curves in a single plot (new in sklearn 0.22)](#trick21)\n\n[Trick 20: Plot confusion matrix (new in sklearn 0.22)](#trick20)\n\n[Trick 19: Most important parameters of a LogisticRegression](#trick19)\n\n[Trick 18: Convert your GridSearchCV or RandomizedGridSearch results into a pandas DataFrame](#trick18)\n\n[Trick 17: RandomizedGridSearch](#trick17)\n\n[Trick 16: Crossvalidate and gridsearch a sklearn pipeline](#trick16)\n\n[Trick 15: OneHotEncoder: tips using it](#trick15)\n\n[Trick 14: Handling missing values](#trick14)\n\n[Trick 13: Examine each step of a Pipeline](#trick13)\n\n[Trick 12: Difference between Pipeline and make_pipeline](#trick12)\n\n[Trick 11: KNNImputer](#trick11)\n\n[Trick 10: Using random_state to reproduce results](#trick10)\n\n[Trick 9: Using missing values as a feature: SimpleImputer & add_indicator = True](#trick9)\n\n[Trick 8: Using make_pipeline in a ML project](#trick8)\n\n[Trick 7: Handle new data while using OneHotEncoder](#trick7)\n\n[Trick 6: Common ways to encode categorical features: OneHotEncoder, OrdinalEncoder](#trick6)\n\n[Trick 5: Benefits of using sklearn for preprocessing and not pandas](#trick5)\n\n[Trick 4: When to use fit_transform and transform methods](#trick4)\n\n[Trick 3: Difference between fit and transform method](#trick3)\n\n[Trick 2: Seven ways to select columns using ColumnsTransformer](#trick2)\n\n[Trick 1: Using column selector to transform different columns](#trick1)\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"Imports\"></a>\n# Importing libraries and setting some helper functions\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# basic libraries\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython.display import Image\n\n# this will allow us to print all the files as we generate more in the kernel\ndef print_files(directory = \"output\"):\n    if directory.lower() == \"input\":\n        for dirname, _, filenames in os.walk('/kaggle/input'):\n            for filename in filenames:\n                print(os.path.join(dirname, filename))\n    else:\n        for dirname, _, filenames in os.walk('/kaggle/working'):\n            for filename in filenames:\n                print(os.path.join(dirname, filename))\n\n# check Trick 91 for an example\ndef generate_sample_data(): # creates a fake df for testing\n    number_or_rows = 20\n    num_cols = 7\n    cols = list(\"ABCDEFG\")\n    df = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\n    df.index = pd.util.testing.makeIntIndex(number_or_rows)\n    return df\n\n# check Trick 91 for an example\ndef generate_sample_data_datetime(): # creates a fake df for testing\n    number_or_rows = 365*24\n    num_cols = 2\n    cols = [\"sales\", \"customers\"]\n    df = pd.DataFrame(np.random.randint(1, 20, size = (number_or_rows, num_cols)), columns=cols)\n    df.index = pd.util.testing.makeDateIndex(number_or_rows, freq=\"H\")\n    return df\n\n# show several prints in one cell. This will allow us to condence every trick in one cell.\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nprint_files(\"input\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick35\"></a>\n# Trick 35: Passing a df directly to sklearn\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# We don't need to use .values when passing a df or a pandas series to sklearn\n# It knows internally how to acess the values and deal with them\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.linear_model import LogisticRegression\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nX = df[['Pclass', 'Fare']]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# check the X and y types\nprint(type(X))\nprint(type(y))\n\n#------------------------------------------------------------\n# instanciate our classes\nmodel = LogisticRegression()\n\n# we fit directly a df and a series and sklearn deals with the rest\nmodel.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick34\"></a>\n# Trick 34: Feature selection with Pipeline\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectPercentile, chi2\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nX = df[\"Name\"]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# instanciate our classes\nvectorizer = CountVectorizer()\nmodel = LogisticRegression()\n\n#------------------------------------------------------------\n# make the pipeline without feature selection\npipe = make_pipeline(vectorizer, model)\nscore = cross_val_score(pipe, X, y, scoring = 'accuracy').mean()\nprint(\"Score of pipeline without feature selection is {}\".format(score))\n\n#------------------------------------------------------------\n# make the pipeline without feature selection\n\n# keep 50% of features with the best chi-squared scores\nselection = SelectPercentile(chi2, percentile = 50)\n\n# add the selection after preprocessing but before model\npipe = make_pipeline(vectorizer, selection, model)\nscore = cross_val_score(pipe, X, y, scoring = 'accuracy').mean()\nprint(\"Score of pipeline with feature selection is {}\".format(score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick33\"></a>\n# Trick 33: Using custom and existing function in a ColumnTransformer\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import FunctionTransformer\n\n#------------------------------------------------------------\n# get some data\nX = pd.DataFrame({'Fare':[200, 300, 50, 900],\n                  'Code':['X12', 'Y20', 'Z7', np.nan],\n                  'Deck':['A101', 'C102', 'A200', 'C300']})\n\n#------------------------------------------------------------\n# use an existing column and make the function compatible with Pipeline\nclip_values = FunctionTransformer(np.clip, kw_args={'a_min':100, 'a_max':600})\n\n#------------------------------------------------------------\n# create a custom function\ndef first_letter(string_column):\n    return string_column.apply(lambda x: x.str.slice(0, 1))\n\n# now use FunctionTransformer to make the function compatible with Pipeline\nget_first_letter = FunctionTransformer(first_letter)\n\n#------------------------------------------------------------\n# create the column Transformer\nct = make_column_transformer(\n    (clip_values, ['Fare']),\n    (get_first_letter, ['Code', 'Deck']))\n\n#------------------------------------------------------------\n# Original X\nprint(\"Original X\")\nX\n\n#------------------------------------------------------------\n# Modified X\nprint(\"Modified X\")\nct.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick32\"></a>\n# Trick 32: Area Under Curve (AUC) for binary classification: ovo and ovr strategies\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.datasets import load_wine\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\n\n# get X and y for classification\nX, y = load_wine(return_X_y=True)\n\n# select only a few features\nX = X[:, 0:2]\n\n# instanciate the model for regression\nmodel_clf = LogisticRegression()\n\n# Multiclass AUC with train/test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nmodel_clf.fit(X_train, y_train)\ny_score = model_clf.predict_proba(X_test)\n\n# use 'ovo' (One-vs-One) or 'ovr' (One-vs-Rest)\nprint(\"roc_auc_score is {} for one vs one strategy with train test split\".format(roc_auc_score(y_test, y_score, multi_class = 'ovo')))\nprint(\"roc_auc_score is {} for one vs rest strategy with train test split\".format(roc_auc_score(y_test, y_score, multi_class = 'ovr')))\nprint(\"-----------------------------------------\")\n\n# Multiclass AUC with cross-validation\n# use 'roc_auc_ovo' (One-vs-One) or 'roc_auc_ovr' (One-vs-Rest)\nprint(\"cross_val_score is {} for one vs one strategy with cross validation\".format(cross_val_score(model_clf, X, y, cv = 5, scoring = 'roc_auc_ovo').mean()))\nprint(\"cross_val_score is {} for one vs rest strategy with cross validation\".format(cross_val_score(model_clf, X, y, cv = 5, scoring = 'roc_auc_ovr').mean()));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick31\"></a>\n# Trick 31: Shuffle when using cross_val_score\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# get X and y for regression\nX_reg, y_reg = load_diabetes(return_X_y = True)\n\n# get X and y for classification\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\", usecols = ['Pclass', 'Fare', 'SibSp', 'Survived']).dropna()\n\n# separate X and y\nX_clf = df[['Pclass', 'Fare', 'SibSp']]\ny_clf = df[[\"Survived\"]]\n\n# instanciate the model for regression\nmodel_reg = LinearRegression()\n\n# instanciate the model for classification\nmodel_clf = LogisticRegression()\n\n# Use KFold for regression\nkf = KFold(5, shuffle = True, random_state = 1)\nprint(\"cross_val_score for regression model\")\ncross_val_score(model_reg, # the regression model, in our case, LinearRegression\n                X_reg, # X: features to learn from\n                y_reg, # y: what the predict\n                cv = kf, # cross_validation scheme we have created earlier\n                scoring = \"r2\") # metric to use to validate the quality of the model\n\n# Use StratifiedKFold for classification\nskf = StratifiedKFold(5, shuffle = True, random_state = 1)\nprint(\"cross_val_score for classification model\")\ncross_val_score(model_clf, # the model, in our case, LogisticRegression\n                X_clf, # X: features to learn from\n                y_clf, # y: what the predict\n                cv = skf, # cross_validation scheme we have created earlier\n                scoring = \"accuracy\") # metric to use to validate the quality of the model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick30\"></a>\n# Trick 30: Four ways of displaying the model coefficients\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n\n#------------------------------------------------------------\n# instanciate the classes\nohe = OneHotEncoder()\nmodel = LogisticRegression()\n\n#------------------------------------------------------------\n# get the data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\", usecols = ['Embarked', 'Survived']).dropna()\n\n# separate X and y\nX = df[[\"Embarked\"]]\ny = df[[\"Survived\"]]\n\n# create a pipeline and fit the X and y\npipe = Pipeline([(\"ohe\", ohe),\n                 (\"clf\", model)])\npipe.fit(X, y)\n\n# inspect the coefficients\nprint(\"1 way to show model coefficients\")\npipe.named_steps.clf.coef_\nprint(\"2 way to show model coefficients\")\npipe.named_steps[\"clf\"].coef_\nprint(\"3 way to show model coefficients\")\npipe[\"clf\"].coef_\nprint(\"4 way to show model coefficients\")\npipe[1].coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick29\"></a>\n# Trick 29: Vectorize two text columns using ColumnTransformer\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\n\n#------------------------------------------------------------\n# get train data\ndf_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\n# drop any nans\nX = df_train[[\"Name\", \"Cabin\"]].dropna()\n\n#------------------------------------------------------------\n# instanciate CountVectorizer\ncount_vect = CountVectorizer()\n\n#------------------------------------------------------------\n# instanciate CountVectorizer\n# You can pass the CountVectorizer multiple times and it will learn\n# separate vocabularies.\n# to do so, you must use make_column_transformer\nct = make_column_transformer((count_vect, 'Name'), (count_vect, 'Cabin'))\nX_transform = ct.fit_transform(X)\nX_transform","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick28\"></a>\n# Trick 28: Save a model of pipeline using joblib\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nimport joblib\n\n#------------------------------------------------------------\n# get train data\ndf_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\n# drop any nans\ndf_train.dropna(axis = \"rows\", inplace = True)\n\n# separate X and y\ncols_for_x = [\"Embarked\", \"Sex\"]\nX_train = df_train[cols_for_x]\ny_train = df_train[\"Survived\"]\n\n#------------------------------------------------------------\n# get test data\ndf_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# drop any nans\ndf_test.dropna(axis = \"rows\", inplace = True)\n\nX_test = df_test[cols_for_x]\n\n#------------------------------------------------------------\n# instanciate Ohe and make_pipeline\nohe = OneHotEncoder()\nmodel = LogisticRegression()\n\n#------------------------------------------------------------\n# create the pipeline\npipe = make_pipeline(ohe, model)\n\n#------------------------------------------------------------\n# predict_using pipeline\npipe.fit(X_train, y_train)\n\n#------------------------------------------------------------\n# save pipeline\njoblib.dump(pipe, 'pipe.joblib')\n\n# print our newly saved pipeline\nprint_files()\n\n#------------------------------------------------------------\n# save pipeline\nnew_pipe = joblib.load('/kaggle/working/pipe.joblib')\n\n#------------------------------------------------------------\n# predict using the same pipe and the old pipe\nprint(\"------------\")\nprint(\"Old pipe.\")\npipe.predict(X_test)\nprint(\"------------\")\nprint(\"Old pipe.\")\nnew_pipe.predict(X_test)\nprint(\"Notice that both pipes predict the same result.\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick27\"></a>\n# Trick 27: Imputing missing values for categorical values\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# get some fake data\nd = {\"Shape_Original\":[\"square\", \"square\", \"square\", \"oval\", \"circle\", np.nan]}\ndf = pd.DataFrame(d)\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import SimpleImputer\n\n#------------------------------------------------------------\n# impute values using most frequent\ndf[\"most_frequent\"] = SimpleImputer(strategy = \"most_frequent\").fit_transform(df[[\"Shape_Original\"]])\n\n#------------------------------------------------------------\n# impute values using most constant\ndf[\"constant\"]  = SimpleImputer(strategy = \"constant\", fill_value = \"missing\").fit_transform(df[[\"Shape_Original\"]])\n\n#------------------------------------------------------------\n# the result of our imputation\ndf.style.apply(lambda x: ['background: lightgreen' if x.name == 5 else '' for i in x], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick26\"></a>\n# Trick 26: Use of stratify when performing classification problems\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import train_test_split\n\n#------------------------------------------------------------\n# generate some data and separate X and y\ndf = pd.DataFrame({'feature':list(range(8)), 'target':['not fraud']*6 + ['fraud']*2})\nX = df[['feature']]\ny = df['target']\n\n#------------------------------------------------------------\n# train and test without stratify\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, random_state = 1)\n\nprint(\"y_train withous stratify\")\ny_train\nprint(\"y_test withous stratify\")\ny_test\n\n\n#------------------------------------------------------------\n# train and test with stratify\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, stratify = y, random_state = 1)\n\nprint(\"Notice how using statify preserves the fraud and not fraud percentage.\")\nprint(\"y_train with stratify\")\ny_train\nprint(\"y_test with stratify\")\ny_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick25\"></a>\n# Trick 25: Prunning decision trees (new in 0.22 and above).\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf['Sex'] = df['Sex'].map({'male':0, 'female':1})\nX = df[[\"Pclass\", \"Fare\", \"Sex\"]]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# basic model and evaluation\nmodel = DecisionTreeClassifier(random_state = 175)\nmodel.fit(X, y)\nscore = cross_val_score(model, X, y, scoring = \"accuracy\")\nprint(\"Our DecissionTree with {} nodes has scored {}\".format(model.tree_.node_count, score.mean()))\n\n#------------------------------------------------------------\n# prun the tree and see cross validation score\n# Notice that the score went up. Prunnig trees has a lot of benefits, the main one is reducing overfitting.\n# ccp_alpha is the parameter that controls the decision tree complexity (cost complexity parameter).\n# Greater values of ccp_alpha increase the number of nodes pruned.\n# documentation \n# https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html\n\nmodel = DecisionTreeClassifier(ccp_alpha = 0.001, random_state = 175)\nmodel.fit(X, y)\nscore = cross_val_score(model, X, y, scoring = \"accuracy\")\nprint(\"Our prunned DecissionTree with {} nodes has scored {}\".format(model.tree_.node_count, score.mean()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick24\"></a>\n# Trick 24: Plotting the decision tree with sklearn (new in 0.21 and above).\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree, export_text\n\n#------------------------------------------------------------\n# create our instances\nmodel = DecisionTreeClassifier()\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf['Sex'] = df['Sex'].map({'male':0, 'female':1})\nX = df[[\"Pclass\", \"Fare\", \"Sex\"]]\ny = df[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Fare\", \"Sex\"]\nclasses = [\"Survived\"]\n\n#------------------------------------------------------------\n# train test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\n#------------------------------------------------------------\n# fit and predict\n\nmodel.fit(X_train, y_train)\n\n#------------------------------------------------------------\n# plot the tree\n\nplt.figure(figsize = (20, 10))\nplot_tree(model, feature_names = features, filled = True);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# show the text\n# I will plot only the first 200 characters of the tree since it grows rapidly\nprint(export_text(model, feature_names = features, show_weights=True)[:200]);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick23\"></a>\n# Trick 23: Display the intercept & coefficients for a liner model\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import LinearRegression\n\n#------------------------------------------------------------\n# load data and separate X and y\ndataset = load_diabetes()\nX, y = dataset.data, dataset.target\nfeatures = dataset.feature_names\n\n#------------------------------------------------------------\n# fit model\nmodel = LinearRegression()\nmodel.fit(X, y)\n\n#------------------------------------------------------------\n# intercept and coef\nmodel.intercept_\nmodel.coef_\nlist(zip(features, model.coef_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick22\"></a>\n# Trick 22: Two types of Pipelines\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# Two types of ROC Curve\n\n# If the pipeline ends in a classifier or regressor, you use the fit and predict methods\n# If the pipeline ends in a transformer you use the fit_transform and transform methods\n\npath = \"/kaggle/input/roc-curve/ROC Curve.jpeg\"\nImage(path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick21\"></a>\n# Trick 21: Several ROC curves in a single plot (new in sklearn 0.22)\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#------------------------------------------------------------\n# create our instances\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\nrf = RandomForestClassifier()\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nX = df[[\"Pclass\", \"Fare\"]]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\n#------------------------------------------------------------\n# fit and predict\nlr.fit(X_train, y_train)\ndt.fit(X_train, y_train)\nrf.fit(X_train, y_train)\n\n#------------------------------------------------------------\n# plot roc curve\ndisp = plot_roc_curve(lr, X_test, y_test)\nplot_roc_curve(dt, X_test, y_test, ax = disp.ax_) # ax = disp.ax_ this line will share the x axis\nplot_roc_curve(rf, X_test, y_test, ax = disp.ax_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick20\"></a>\n# Trick 20: Plot confusion matrix (new in sklearn 0.22)\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\n\n#------------------------------------------------------------\n# create our instances\nmodel = LogisticRegression(random_state = 1)\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nX = df[[\"Pclass\", \"Fare\"]]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# train test split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n\n#------------------------------------------------------------\n# fit and predict\n\nmodel.fit(X_train, y_train)\n\n#------------------------------------------------------------\n# plot confusion matrix\n# notice that you have to pass the model, X_test and y_test\n# plot_confusion_matrix predicts with the model and plots the values\n\ndisp = plot_confusion_matrix(model, X_test, y_test, cmap = \"Blues\", values_format = \".3g\")\n\nprint(\"The classical confusion matrix\")\ndisp.confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick19\"></a>\n# Trick 19: Most important parameters of a LogisticRegression\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# C: inverse of regularization strength\n# penalty: type of regularization\n# solver: algorithm used for optimization\n\npath = \"/kaggle/input/logisticregression/LogisticRegression.jpg\"\nImage(path)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick18\"></a>\n# Trick 18: Convert your GridSearchCV or RandomizedGridSearch results into a pandas DataFrame\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n#------------------------------------------------------------\n# create our instances\nohe = OneHotEncoder()\nvect = CountVectorizer()\nct = make_column_transformer((ohe, [\"Sex\"]), (vect, \"Name\"))\nmodel = LogisticRegression(solver = \"liblinear\", random_state = 1)\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nX = df[[\"Sex\", \"Name\", \"Fare\"]]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# make pipeline\npipeline = make_pipeline(ct, model)\n\n#------------------------------------------------------------\n# cross validate the entire pipeline\nprint(\"Notice the score of our entire pipeline is {}\".format(cross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()))\ncross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()\n\n#------------------------------------------------------------\n# gridsearch the entire pipeline\n\n# set the parameters\nparams = {\"columntransformer__countvectorizer__min_df\":[1, 2],\n         \"logisticregression__C\":[0.1, 1, 10],\n         \"logisticregression__penalty\":[\"l1\", \"l2\"]}\n\ngrid = GridSearchCV(pipeline, params, cv = 5, scoring = \"accuracy\")\ngrid.fit(X, y)\n\n# convert to a pandas DataFrame\n\nresults = pd.DataFrame(grid.cv_results_)[[\"params\", \"mean_test_score\", \"rank_test_score\"]]\nresults.sort_values(\"rank_test_score\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick17\"></a>\n# Trick 17: RandomizedGridSearch\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\nimport scipy as sp\n\n#------------------------------------------------------------\n# create our instances\nvect = CountVectorizer()\nmodel = MultinomialNB()\n\n#------------------------------------------------------------\n# make pipeline\npipeline = make_pipeline(vect, model)\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nX = df['Name']\ny = df['Survived']\n\n#------------------------------------------------------------\n# set the params to optimize\n\nparams = {}\n\nparams[\"countvectorizer__min_df\"] = [1, 2, 3, 4]\nparams[\"countvectorizer__lowercase\"] = [True, False]\nparams[\"multinomialnb__alpha\"] = sp.stats.uniform(scale = 1)\n\n#------------------------------------------------------------\n# optimize\n\nrand = RandomizedSearchCV(pipeline, params, n_iter = 10, cv = 5, scoring = \"accuracy\", random_state = 1)\nrand.fit(X, y)\n\n#------------------------------------------------------------\n# best score and params\n\nprint(\"Best score achieved with our search is:\")\nrand.best_score_\n\nprint(\"Best params are:\")\nrand.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick16\"></a>\n# Trick 16: Crossvalidate and gridsearch a sklearn pipeline\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\n#------------------------------------------------------------\n# create our instances\nohe = OneHotEncoder()\nvect = CountVectorizer()\nct = make_column_transformer((ohe, [\"Sex\"]), (vect, \"Name\"))\nmodel = LogisticRegression(solver = \"liblinear\", random_state = 1)\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nX = df[[\"Sex\", \"Name\", \"Fare\"]]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# make pipeline\npipeline = make_pipeline(ct, model)\n\n#------------------------------------------------------------\n# cross validate the entire pipeline\nprint(\"Notice the score of our entire pipeline is {}\".format(cross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()))\ncross_val_score(pipeline, X, y, cv = 5, scoring = \"accuracy\").mean()\n\n#------------------------------------------------------------\n# gridsearch the entire pipeline\n\n# set the parameters\nparams = {\"columntransformer__countvectorizer__min_df\":[1, 2],\n         \"logisticregression__C\":[0.1, 1, 10],\n         \"logisticregression__penalty\":[\"l1\", \"l2\"]}\n\ngrid = GridSearchCV(pipeline, params, cv = 5, scoring = \"accuracy\")\ngrid.fit(X, y)\n\n# see the best score\nprint(\"#-------------------------------------------------------------------------\")\nprint(\"Best score of the GridSearchCV is \")\ngrid.best_score_\n\n# see the best params\nprint(\"#-------------------------------------------------------------------------\")\nprint(\"Best parameters of the GridSearchCV are \")\ngrid.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick15\"></a>\n# Trick 15: OneHotEncoder: tips using it\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"markdown","source":"If you use OneHotEncoder, don't \"drop = 'first'\":\n1. Multicollinearity is rarely and issue with sklearn models\n2. drop = 'first' is incompatible with handle_unknown = 'ignore'\n3. May cause you problems if yoy standarize all features or use a regularized model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick14\"></a>\n# Trick 14: Handling missing values\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# If you have missing values you can:\n# 1. Drop all rows with missing values\n# 2. Drop all colmns with missing values\n# 3. Impute missing values\n# 4. Use a model that handles missing values\n\n#------------------------------------------------------------\n# import libraries\nfrom sklearn.experimental import enable_hist_gradient_boosting # this import enables \"experimental\" packages and clases in sklearn\nfrom sklearn.ensemble import HistGradientBoostingClassifier # this is an experimental package\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nprint(\"We can see that we have missing values\")\n\ndf.isnull().sum()\n\n#------------------------------------------------------------\n# split target and feature\n\nfeatures = [col for col in df.columns if df[col].dtype != \"object\"] # select only numerical columns\nfeatures.remove(\"Survived\")\nfeatures.remove(\"PassengerId\")\n\nX = df[features]\ny = df[\"Survived\"]\n\n#------------------------------------------------------------\n# Train a model that handles missing values\n\nmodel = HistGradientBoostingClassifier()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 175)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\nprint(\"Accuracy of our model while having missing values is {}%\".format(round(accuracy_score(y_test, y_pred), 2)*100))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick13\"></a>\n# Trick 13: Examine each step of a Pipeline\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\n#------------------------------------------------------------\n# create each instance\nsi = SimpleImputer()\nmodel = LogisticRegression()\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\n#------------------------------------------------------------\n# select columns to transform\nX = df[[\"Fare\", \"Age\"]].head()\nX[\"Age\"].iloc[0] = np.nan # create a missing values\nX.head()\n\ny = df[[\"Survived\"]].head()\n\n#------------------------------------------------------------\n# use make_pipeline\n\npipeline = make_pipeline(si, model)\n\npipeline.fit(X, y)\n\n#------------------------------------------------------------\n# let's see the statistics of each step\nprint(\"These are the imputed values with the SimpleImputer\")\npipeline.named_steps.simpleimputer.statistics_\n\nprint(\"Display the coefficients of the linear model\")\npipeline.named_steps.logisticregression.coef_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick12\"></a>\n# Trick 12: Difference between Pipeline and make_pipeline\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.compose import make_column_transformer, ColumnTransformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n#------------------------------------------------------------\n# create each instance\nohe = OneHotEncoder()\nsi = SimpleImputer()\nmodel = LogisticRegression()\n\n#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\n#------------------------------------------------------------\n# select columns to transform\nX = df[[\"Fare\", \"Embarked\", \"Sex\", \"Age\"]].head()\nX[\"Age\"].iloc[0] = np.nan # create a missing values\nX.head()\n\ny = df[[\"Survived\"]].head()\n\n#------------------------------------------------------------\n# use make_pipeline\n\ncolumn_transformer = make_column_transformer(\n(ohe, [\"Embarked\", \"Sex\"]),\n(si, [\"Age\"]),\nremainder = \"passthrough\"\n)\n\npipeline = make_pipeline(column_transformer, model)\n\npipeline.fit(X, y)\n\n#------------------------------------------------------------\n# use Pipeline\n# The main difference is that we must name each step\n\ncolumn_transformer = ColumnTransformer(\n[(\"encoder\", ohe, [\"Embarked\", \"Sex\"]), # notice how we must name each step\n(\"imputer\", si, [\"Age\"])],\nremainder = \"passthrough\"\n)\n\npipeline = Pipeline([(\"preprocessing\", column_transformer), (\"model\", model)]) # notice how we must name each step\npipeline.fit(X, y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick11\"></a>\n# Trick 11: KNNImputer\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.impute import KNNImputer\n\n#------------------------------------------------------------\n# create some random data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf.isnull().sum() # inspect nulls\ndf.head()\n\n#------------------------------------------------------------\n# let's see KNNInputer in action\nknn_inputer = KNNImputer()\n\nX = df[[\"SibSp\", \"Fare\", \"Age\"]]\nnan_index = X[X[\"Age\"].isnull()].index\n\nprint(\"Data with nans\")\nX[X.index.isin(nan_index)].head(10)\n\nprint(\"Transformed data with no nans, and the values are based on the NN.\")\nX_transformed = pd.DataFrame(knn_inputer.fit_transform(X), columns = [\"SibSp\", \"Fare\", \"Age\"], index = X.index)\nX_transformed[X.index.isin(nan_index)].head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick10\"></a>\n# Trick 10: Using random_state to reproduce results\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import libraries\nfrom sklearn.model_selection import train_test_split\n\n#------------------------------------------------------------\n# create some random data\n\ndf = generate_sample_data()\ntarget = [np.random.choice([0, 1]) for i in range(len(df))]\ndf[\"target\"] = target\nprint(\"A sneak peak at our df.\")\ndf.head(3)\n\n#------------------------------------------------------------\n# train and target columns\nfeatures = list(df.columns)[:-1] # all except the last one\n\n#------------------------------------------------------------\n# split nr 1 using random_state to reproduce results\nX_train1, X_test1, y_train1, y_test1 = train_test_split(df[features], df[\"target\"], test_size = 0.1, random_state = 175)\n\n#------------------------------------------------------------\n# split nr 2 using random_state to reproduce results\nX_train2, X_test2, y_train2, y_test2 = train_test_split(df[features], df[\"target\"], test_size = 0.1, random_state = 175)\n\n#------------------------------------------------------------\n# split nr 3 using with no random_state\nX_train3, X_test3, y_train3, y_test3 = train_test_split(df[features], df[\"target\"], test_size = 0.1)\n\n#------------------------------------------------------------\n# let's look at our results\nprint(\"First train test split.\")\nX_train1.head()\nprint(\"Second train test split. Equal to the first one.\")\nX_train2.head()\nX_train1.index == X_train2.index\nprint(\"Third train test split. Different than the others.\")\nX_train3.head()\nX_train1.index == X_train3.index\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick9\"></a>\n# Trick 9: Using missing values as a feature: SimpleImputer & add_indicator = True\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import the libraries\nfrom sklearn.impute import SimpleImputer\n\n#------------------------------------------------------------\n# create some train and test data\n\n# create a train df\nd = {\n\"Age\":[10, 20, np.nan, 30, 15, 10, 40, 10, np.nan]\n}\n\nprint(\"Train data\")\ndf_train = pd.DataFrame(d)\ndf_train\n\n#------------------------------------------------------------\n# Sometimes there is a relathionship between missing values and the target\n# We can use this information creating a new features while performing an imputation of a missing values\n\n# normal SimpleImputer()\nimputer = SimpleImputer()\ndf_transformed = imputer.fit_transform(df_train)\ndf_transformed\n\n# SimpleImputer() with the parameter add_indicator\nprint(\"Notice aditional column with an indicator of 1 next to the previously missing values\")\nimputer = SimpleImputer(add_indicator = True)\ndf_transformed = imputer.fit_transform(df_train)\ndf_transformed\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick8\"></a>\n# Trick 8: Using make_pipeline in a ML project\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import the libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\n#------------------------------------------------------------\n# create some train and test data\n\n# create a train df\nd = {\n\"feat1\":[10, 20, np.nan, 2],\n\"feat2\":[25, 20, 5, 3],\n\"target\":[\"A\", \"A\", \"B\", \"B\"]\n}\n\nprint(\"Train data\")\ndf_train = pd.DataFrame(d)\ndf_train\n\n# create a test df\nd = {\n\"feat1\":[30, 5, 15],\n\"feat2\":[12, 10, np.nan]\n}\n\nprint(\"Test data\")\ndf_test = pd.DataFrame(d)\ndf_test\n\n#------------------------------------------------------------\n# simple ML project step by step\n\n# create each instance\nsi = SimpleImputer()\nmodel = LogisticRegression()\npipeline = make_pipeline(si, model)\n\n# separate the data between target and features\nfeatures = [\"feat1\", \"feat2\"]\nX_train, y_train = df_train[features], df_train[\"target\"]\nX_test = df_test[features]\n\n#------------------------------------------------------------\n# use pipeline to fit and predict\n\n# First pipeline will use the SimpleImputer to imputer the missing values\n# Then it will train using LogisticRegression\npipeline.fit(X_train, y_train)\n\n# When used pipeline to predict, it will do the same steps as in fit\npipeline.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick7\"></a>\n# Trick 7: Handle new data while using OneHotEncoder\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# create a train df\nd = {\n\"Categorical\":[\"A\", \"A\", \"B\", \"C\"]\n}\n\ndf_train = pd.DataFrame(d)\ndf_train\n\n#------------------------------------------------------------\n# import the libraries\nfrom sklearn.preprocessing import OneHotEncoder\n\n#------------------------------------------------------------\n# transform data during train part\nprint(\"fit_transform using OneHotEncoder.\")\nohe = OneHotEncoder(sparse = False, handle_unknown = \"ignore\") # if you don't put false, you will get a sparse matrix object\nX_train = ohe.fit_transform(df_train[[\"Categorical\"]])\nX_train\n\n#------------------------------------------------------------\n# create a test df\n\nd = {\n\"Categorical\":[\"A\", \"A\", \"B\", \"C\", \"D\"] # new value, D, previously not seen in train\n}\n\ndf_test = pd.DataFrame(d)\ndf_test\n\n\nprint(\"transform using OneHotEncoder. Notice that we have a line with zeros for categorical value of D\")\nX_test = ohe.transform(df_test[[\"Categorical\"]])\nX_test\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick6\"></a>\n# Trick 6: Common ways to encode categorical features: OneHotEncoder, OrdinalEncoder\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# create a df\nd = {\n\"Shape\":[\"square\", \"square\", \"oval\", \"circle\"],\n\"Class\":[\"third\", \"first\", \"second\", \"third\"],\n\"Size\":[\"S\", \"S\", \"L\", \"XL\"]\n}\n\ndf = pd.DataFrame(d)\ndf\n\n#------------------------------------------------------------\n# import the libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\n\n#------------------------------------------------------------\n# transform data using OneHotEncoder\nprint(\"Transform categorical data using OneHotEncoder\")\nohe = OneHotEncoder(sparse = False) # if you don't put false, you will get a sparse matrix object\nshaped_transformed = ohe.fit_transform(df[[\"Shape\"]]) # if you pass as a series, you will need to reshape the data. Notice the double square bracket\nshaped_transformed\n\n#------------------------------------------------------------\n# transform data using OrdinalEncoder\nprint(\"Transform categorical data using OrdinalEncoder\")\nprint(\"When using OrdinalEncoder, your data has to have a order: like first class, second class, third class\")\noe = OrdinalEncoder(categories = [[\"first\", \"second\", \"third\"], # order for the column Class\n                                  [\"S\", \"M\", \"L\", \"XL\"]]) # order for the column Size\ncategorical_ordinal_transformed = oe.fit_transform(df[[\"Class\", \"Size\"]])\ncategorical_ordinal_transformed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick5\"></a>\n# Trick 5: Benefits of using sklearn for preprocessing and not pandas\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### Reasons to use sklearn to ML preprocessing and not pandas\n\n1. You can cross-validate the entire workflow\n2. You can grid search model & preprocessing hyperparameters\n3. Avoids adding new columns to the source DataFrame\n4. pandas lacks separate fit/transform steps to prevent data leakage","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick4\"></a>\n# Trick 4: When to use fit_transform and transform methods\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### ***Use \"fit_transform\" on training data, but \"transform\" (only) on testing/new data.***\n\nApplies the same transformations to both sets of data, which creates consistent columns and prevents data leakage!","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick3\"></a>\n# Trick 3: Difference between fit and transform method\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":false},"cell_type":"markdown","source":"### Q: What is the difference between the \"fit\" and \"transform\" methods?\n\n#### ***\"fit\"***: transformer learns something about the data\n\n#### ***\"transform\":*** it uses what it learned to do the data transformation\n\n------\n\n***CountVectorizer: ***\n\nfit: learns the vocabulary\n\ntransform: creates a document term matrix using the vocabulary\n\n------\n\n***SimpleImputer: ***\n\nfit: learns the value to impute\n\ntransform: fills the missing value with the value to impute\n\n------\n\n***StandartScaler: ***\n\nfit: learns the mean and scale of each feature\n\ntransform: standarizes the features using the mean and scale\n\n------\n\n***HashingVectorizer: ***\n\nfit: if not used, then it's known as \"stateless\" transformer\n\ntransform: creates a document term matrix using a hash of the token\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick2\"></a>\n# Trick 2: Seven ways to select columns using ColumnsTransformer\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n\n#------------------------------------------------------------\n# select columns to transform\nX = df[[\"Fare\", \"Embarked\", \"Sex\", \"Age\"]].head()\nX[\"Age\"].iloc[0] = np.nan # create a missing values\nX\n\n#------------------------------------------------------------\n# import the libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_selector\nfrom sklearn.compose import make_column_transformer\n\n#------------------------------------------------------------\n# instanciate the classes\nohe = OneHotEncoder()\n\n#------------------------------------------------------------\n# create the pipeline and select the columns by name\n\nprint(\"Select the column by name\")\n\nct = make_column_transformer(\n(ohe, [\"Embarked\", \"Sex\"])# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns by position\n\nprint(\"Select the column by position\")\n\nct = make_column_transformer(\n(ohe, [1, 2])# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns using slice\n\nprint(\"Select the column using slice\")\n\nct = make_column_transformer(\n(ohe, slice(1, 3))# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns using a boolean mask\n\nprint(\"Select the column using a boolean mask\")\n\nct = make_column_transformer(\n(ohe, [False, True, True, False])# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns using make_column_selector and regex\n\nprint(\"Select the column using using make_column_selector and regex. New in pandas 0.22\")\n\nct = make_column_transformer(\n(ohe, make_column_selector(pattern = \"E|S\"))# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns using make_column_selector and dtype_include\n\nprint(\"Select the column using using make_column_selector and dtype_include. New in pandas 0.22\")\n\nct = make_column_transformer(\n(ohe, make_column_selector(dtype_include = object))# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed\n\n#------------------------------------------------------------\n# create the pipeline and select the columns using make_column_selector and dtype_exclude\n\nprint(\"Select the column using using make_column_selector and dtype_exclude. New in pandas 0.22\")\n\nct = make_column_transformer(\n(ohe, make_column_selector(dtype_exclude = \"number\"))# if you have null values it will give and error. You must first fill those values before doing an ohe\n)\n\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id = \"trick1\"></a>\n# Trick 1: Using ColumnTransformer to manipulate different columns\n[Go back to the Table of Contents](#table_of_contents)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#------------------------------------------------------------\n# import data\ndf = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf.head()\n\n#------------------------------------------------------------\n# select columns to transform\nX = df[[\"Fare\", \"Embarked\", \"Sex\", \"Age\"]].head()\nX[\"Age\"].iloc[0] = np.nan # create a missing values\nX\n\n#------------------------------------------------------------\n# import the libraries\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import make_column_transformer\n\n#------------------------------------------------------------\n# instanciate the classes\nohe = OneHotEncoder()\nimp = SimpleImputer()\n\n#------------------------------------------------------------\n# create the pipeline\nct = make_column_transformer(\n(ohe, [\"Embarked\", \"Sex\"]), # if you have null values it will give and error. You must first fill those values before doing an ohe\n(imp, [\"Age\"]), \nremainder = \"passthrough\" # this means that the column Fare will appear the last one.\n)\n\n#------------------------------------------------------------\n# fit_transform the columns\nX_transformed = ct.fit_transform(X) \nX_transformed","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# The End\n# Thanks a lot. If you made till the end you have learned a lot of sklearn","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}